import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import OllamaEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import Ollama
import tempfile
import os

st.set_page_config(page_title="üá∞üáø –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –†–ö ‚Äî AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")

st.title("üá∞üáø –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –†–ö ‚Äî AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")
st.markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ .txt —Ñ–∞–π–ª –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏:")

uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", type=["txt"])

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–∞
if uploaded_file:
    st.success(f"–§–∞–π–ª {uploaded_file.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ TXT!")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".txt", mode='w', encoding="utf-8") as tmp_file:
        content = uploaded_file.read().decode("utf-8")
        tmp_file.write(content)
        tmp_filepath = tmp_file.name

    loader = TextLoader(tmp_filepath, encoding="utf-8")
    documents = loader.load()

    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = splitter.split_documents(documents)

    embeddings = OllamaEmbeddings(model="llama3")  # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ mistral
    vectorstore = FAISS.from_documents(texts, embeddings)

    st.session_state.vectorstore = vectorstore
    os.remove(tmp_filepath)  # —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

# –ü–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ –≤–æ–ø—Ä–æ—Å–∞
question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏:")

if question:
    if "vectorstore" not in st.session_state:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–µ–π.")
    else:
        llm = Ollama(model="llama3")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=st.session_state.vectorstore.as_retriever(),
            return_source_documents=True
        )

        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç..."):
            result = qa_chain.invoke({"query": question})

        st.markdown("### üìå –û—Ç–≤–µ—Ç:")
        st.write(result["result"])

        with st.expander("üîç –ò—Å—Ç–æ—á–Ω–∏–∫–∏:"):
            for doc in result["source_documents"]:
                st.markdown(f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}")
                st.markdown(doc.page_content[:500] + "...")
